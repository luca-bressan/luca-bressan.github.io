<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-12-25T21:04:02+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Luca Bressan - finance &amp;amp; statistics</title><subtitle>My finance &amp; statistics blog</subtitle><entry><title type="html">‚ÄúHe‚Äôs giving away the numbers!‚Äù: facts about numerical integration (Numerical interpolation, part 1)</title><link href="http://localhost:4000/he-s-giving-the-numbers" rel="alternate" type="text/html" title="‚ÄúHe‚Äôs giving away the numbers!‚Äù: facts about numerical integration (Numerical interpolation, part 1)" /><published>2022-12-03T02:00:00+01:00</published><updated>2022-12-03T02:00:00+01:00</updated><id>http://localhost:4000/he-s-giving-the-numbers</id><content type="html" xml:base="http://localhost:4000/he-s-giving-the-numbers">&lt;p&gt;Cover photo by &lt;a href=&quot;https://unsplash.com/@waldemarbrandt67w?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Waldemar Brandt&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/s/photos/lottery-extraction?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Unsplash&lt;/a&gt;. In Italy, when someone is speaking nonsense or is saying something outrageously wrong, we say that he‚Äôs &lt;em&gt;‚Äúgiving away the numbers‚Äù&lt;/em&gt;. Hopefully we‚Äôll be able to retain our mental fitness and say a lot of very correct stuff, while still giving away some numbers in a literal sense.&lt;/p&gt;

&lt;h3 id=&quot;what-you-should-know-before-reading-this-post&quot;&gt;What you should know before reading this post&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Basic linear algebra&lt;/li&gt;
  &lt;li&gt;Basic calculus&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-quest-for-numerical-integration&quot;&gt;The quest for numerical integration&lt;/h1&gt;

&lt;p&gt;There comes a point where our nice theoretical results, with closed formulas and all that jazz, are no longer sufficient to make a decision, give an estimate or complete our task. Sometimes we just need a number, a nicely written one using only digits, commas and some scientific notation for the adventurous. A particular class of problems that arises in obtaining our dear number is the one of &lt;em&gt;numerical integration&lt;/em&gt;. Riemann and Lebesgue integrals are &lt;strong&gt;not&lt;/strong&gt; particularly wieldy objects to begin with &lt;em&gt;(that‚Äôs the reason why they‚Äôre plenty of fun, no?)&lt;/em&gt; and Ito integrals &lt;strong&gt;are even worse&lt;/strong&gt;. In fact, for Ito intergrals, it‚Äôs quite hard to even state what a solution written with just digits is. Do we want a &lt;em&gt;path-wise solution&lt;/em&gt; or do we just need &lt;em&gt;some statistical properties to hold?&lt;/em&gt; We will try to get a grasp of all this tough stuff. But every nice journey begins with, well‚Ä¶ the beginning. &lt;strong&gt;This is part of a series of 3 articles:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;üôã‚Äç‚ôÇÔ∏è &lt;em&gt;Numerical (algebraic) interpolation&lt;/em&gt;, the basis for numerical integration;&lt;/li&gt;
  &lt;li&gt;Numerical approximation of Riemann integrals;&lt;/li&gt;
  &lt;li&gt;Numerical approximation of Ito integrals, specifically targeting the issue of &lt;em&gt;finding a weakly converging approximation scheme&lt;/em&gt; for processes built on the browninan motion (spoiler: this is what we need to price our beloved financial products).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This time we are going to face the task of algebraic interpolation. While this article is going to be pretty far from being exhaustive &lt;em&gt;(to be honest, there are 300 pages long books about interpolation which are pretty far from being exhaustive)&lt;/em&gt;, it will serve as a starting point for our journey and I hope it will stimulate your curiosity. Well, enough preambles: &lt;em&gt;1, 2, 3, take a deep breath and‚Ä¶ let‚Äôs get going!&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-even-is-interpolation&quot;&gt;What even is interpolation?&lt;/h3&gt;

&lt;p&gt;The problem of interpolation arises when we want to find a function \(f(x)\) such that given a point cloud \((x_{i},y_{i})_{i = 1, 2, 3, \dots}\) we have \(f(x_{i})=y_{i}\). In particular, &lt;em&gt;algebraic interpolation is the issue of finding \(f\) among the family of polynomials or piecewise polynomial functions&lt;/em&gt;. When the function is only piecewise polynomial, we are talking about &lt;em&gt;spline interpolation&lt;/em&gt;. A case of particular interest is when there exists a known function \(\varphi(x)\) such that \(y = \varphi(x)\), where \(\varphi(x)\) has some unpleasant properties that make it difficult to evaluate. Another scenario where interpolation is useful is when \(\varphi(x)\) has some known properties but doesn‚Äôt allow for an explicit closed form. In this case we may still want to use a polynomial to approximate \(\varphi\), since &lt;em&gt;polynomials are easy to handle and evaluate&lt;/em&gt;. &lt;strong&gt;There will be errors in doing this:&lt;/strong&gt; we are going to measure them using the \(\infty\)-norm.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;definition-approximation-error&quot;&gt;Definition. &lt;em&gt;(Approximation error)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given an interval \(I \subset \mathbb{R}\) and a function \(\varphi: I \to \mathbb{R}\) we define the &lt;em&gt;approximation error&lt;/em&gt; obtained approximating \(\varphi\) over \(I\) using a polynomial \(f\), \(E(f;\varphi)_{I}\) as&lt;/p&gt;

&lt;p&gt;\begin{equation}
              E_{I}(f; \varphi) = \| f - \varphi \| _{ \infty } \restriction _{I}
\end{equation}&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;This is our first glimpse of the essential nature of numerical analysis: knowing that you are wrong and by how much is even more important than the number you get at the end. Just as a note, sometimes you may mitigate this limitation of numerical analysis using tools like &lt;em&gt;symbolic machine algebra&lt;/em&gt;.
In theory, we can be quite successful at approximating functions by polynomials, we just need our functions to be continuous:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem-weierstrass-theorem-on-approximation-errors&quot;&gt;Theorem. &lt;em&gt;(Weierstrass Theorem on approximation errors)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let \(\varphi\) be a continuous function on some interval \(I\). Then for any \(\varepsilon &amp;gt; 0\) there exists \(n \in \mathbb{N}\) and a polynomial \(P_n\) of degree \(n\) such that \(E_{I}(P_n; \varphi) &amp;lt; \varepsilon\)&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;&lt;em&gt;J-L. Lagrange in 1795&lt;/em&gt; had the brilliant idea to define a vector space of polynomials with degree equal to the number of points in the cloud minus one. Lagrange polynomials have the nice property of &lt;strong&gt;having unit value&lt;/strong&gt; on the various \(x_{i}\), and &lt;strong&gt;they constitute a basis&lt;/strong&gt; for the linear space of polynomials, so that the interpolating polynomial is just a vector whose components are the \(y_{i}\). They look as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;definition-lagrange-polynomials&quot;&gt;Definition. &lt;em&gt;(Lagrange polynomials)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given a point cloud \((x_{i},y_{i})_{i = 1, 2, 3, \dots, n}\) we define the &lt;em&gt;Lagrange basis&lt;/em&gt; of the \(n\)-degree polynomials space as&lt;/p&gt;

&lt;p&gt;\begin{equation}
              l_{k}=\frac{\pi_{n}(x)}{\pi_{n}‚Äô(x) (x-x_{k})}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\pi_{n}=(x-x_{1})(x-x_{2}) \dots (x-x_{n})\).&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;Evaluating the Lagrange interpolating polynomial \(L_{n}=y_{1}l_{1} + \dots + y_{n}l_{n}\) has computational complexity \(\mathcal{O}(n^{2})\), which is quite nice. Anything polynomial is actually nice, NP-hard problems are a massive issue.&lt;/p&gt;

&lt;h3 id=&quot;where-should-i-put-my-x_i&quot;&gt;Where should I put my \(x_{i}\)?&lt;/h3&gt;

&lt;p&gt;Choosing where to evaluate the ‚Äúhard‚Äù function before approximation &lt;em&gt;plays a massive role&lt;/em&gt; in how good our interpolation will be. In fact we have this theorem:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem-first-fabers-theorem&quot;&gt;Theorem. &lt;em&gt;(First Faber‚Äôs theorem)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;For any &lt;strong&gt;a priori&lt;/strong&gt; known algorithm for determining the \(x_{i}\) &lt;em&gt;independently of the target function&lt;/em&gt; \(\varphi\), there exist both a target function such that the approximation error &lt;em&gt;does not converge&lt;/em&gt; to \(0\) as the number of \(x_{i}\) increases and one such that the approximation &lt;em&gt;does indeed converge&lt;/em&gt; to \(0\) as \(n\) goes to infinity.&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;So, no matter how smart we are in pre-determining the \(x_{i}\) (from here on called nodes), &lt;em&gt;sometimes we get things terribly wrong&lt;/em&gt;. But we can still be pretty smart. The following theorem tells us that we can &lt;em&gt;bound the error by means of a recursive relation&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem-lebesgue-constants-theorem&quot;&gt;Theorem. &lt;em&gt;(Lebesgue constants theorem)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let \(\varphi\) be a continuous target function on some interval \(I\). Then the following relation holds&lt;/p&gt;

&lt;p&gt;\begin{equation}
              E_{I}(L_{n}; \varphi) \leq (1+\Lambda_{n}((x_{i})_{i = 1, \dots, n})) \mathcal{E}_{n-1}(\varphi)
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(\Lambda_{n} = \| \lambda_{n} \| _{\infty} \restriction_{I}\) are called Lebesgue‚Äôs constants, the \(n\)-th Lebesgue function \(\lambda_{n}\) is the sum of the absolute values of the Lagrange basis polynomials and \(\mathcal{E}(\varphi)\) is the best possible polynomial approximation of \(\varphi\) using polynomials of degree \(n\).&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;Given the recursive nature of this error bound, we would like to find an algorithm for selecting the nodes that allows us to have the Lebesgue constants grow as slowly as possible as we increase the number of nodes. There is, however, a &lt;em&gt;limit to how slowly we can have them grow&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem-second-fabers-theorem&quot;&gt;Theorem. &lt;em&gt;(Second Faber‚Äôs theorem)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;For any choice of node system, we have \(\Lambda_{n}((x_{i})_{i = 1, \dots, n}) \geq \frac{1}{12} \log (n)\)&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;Therefore, we have that &lt;em&gt;if Lebesgue constants grow logarithmically, the node system has optimal scaling&lt;/em&gt;. Unsurprisingly, the smoothness of the target function also plays an important role.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem&quot;&gt;Theorem&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let \(\varphi\) be a \(k\)-times differentiable target function on some interval \(I\). Then the following relation holds&lt;/p&gt;

&lt;p&gt;\begin{equation}
              E_{I}(L_{n}; \varphi) \leq c \frac{\log n}{n^k}
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(c\) is independent of \(k\) and \(n\)&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;This is it for the general facts. Let‚Äôs have a small recap befor moving on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;interpolating basically means joining points in a nice way. &lt;em&gt;And polynomials are &lt;strong&gt;really&lt;/strong&gt; nice&lt;/em&gt;;&lt;/li&gt;
  &lt;li&gt;Weierstrass‚Äô theorem makes it looks like algebraic interpolation can in theory be &lt;strong&gt;infinitely effective&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;Lagrange gave us a way of choosing the coefficients for our interpolating polynomial &lt;em&gt;that just works&lt;/em&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;we can estimate errors&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;Faber‚Äôs theorems shattered our dreams of a &lt;em&gt;universally good way of picking the interpolation nodes&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our main focus is the choice of the node system, let‚Äôs see some of those and let‚Äôs try to say something about their performance.&lt;/p&gt;

&lt;h3 id=&quot;some-notable-node-systems&quot;&gt;Some notable node systems&lt;/h3&gt;

&lt;p&gt;The first thing we would like to do is to use evenly spaced nodes across our interval. This seemingly reasonable choice is, unfortunately, &lt;strong&gt;an awful one&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem-tureckij-theorem&quot;&gt;Theorem. &lt;em&gt;(Tureckij theorem)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;For an evenly spaced system of nodes the Lebesgue constants have the following scaling:&lt;/p&gt;

&lt;p&gt;\begin{equation}
              \Lambda _ {n} ((x_{i})_{i = 1, \dots, n})) \sim \frac{2^{n}}{e n \log n}
\end{equation}&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;This is related to an unpleasant phenomenon that we are going to see later, called the &lt;em&gt;Runge effect&lt;/em&gt;. This leads many practitioners to &lt;em&gt;limit the usage of evenly spaced nodes only with polynomials of degree at most equal to &lt;strong&gt;nine&lt;/strong&gt;&lt;/em&gt;.
We have smarter choices available, but first we need some extra notions.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;definition-orthogonal-polynomials&quot;&gt;Definition. &lt;em&gt;(Orthogonal polynomials)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Two degree \(n\) polynomials \(p\) and \(q\) defined on the interval \([-1,1]\) are said to be &lt;em&gt;orthogonal&lt;/em&gt; with respect to the \(\alpha , \beta\)-Jacobi weighted &lt;em&gt;inner product&lt;/em&gt; if they satisfy the following condition&lt;/p&gt;

&lt;p&gt;\begin{equation}
              \int_{-1}^{1} p(x) q(x) (1-x)^{\alpha}(1+x)^{\beta} dx = 0
\end{equation}&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;Note that &lt;em&gt;we can always traslate our approximation problem from a generic bounded interval into&lt;/em&gt; \([-1,1]\) &lt;em&gt;by means of an affine transform&lt;/em&gt;. The roots of the orthogonal polynomial defined as before have several nice properties. We will refer to the node system given by these roots as \(w^{\alpha , \beta}\).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem-szeg≈ë-theorem&quot;&gt;Theorem. &lt;em&gt;(Szeg≈ë theorem)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;If \(\alpha\) and \(\beta\) belong in \((-1,\frac{1}{2}]\), then the &lt;em&gt;scaling of the Lebesgue constants&lt;/em&gt; of \(w^{\alpha , \beta}\) &lt;strong&gt;is logarithmic&lt;/strong&gt;, and it is polynomial otherwise.&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;Szeg≈ë theorem therefore states &lt;em&gt;how to reach that optimal logarithmic scaling of the error bound we were looking for&lt;/em&gt;. Sometimes, a process called &lt;em&gt;enrichment&lt;/em&gt; is needed even when we are within the conditions stated by the theorem. We won‚Äôt talk about this process, but it basically is a way of &lt;em&gt;adding nodes to improve performance&lt;/em&gt;. We will perform practical tests on two system of nodes of particular importance: the evenly spaced nodes &lt;em&gt;(that we will indicate with&lt;/em&gt; \(\mathcal{E}\) &lt;em&gt;)&lt;/em&gt; and the the roots of the Chebyshev polynomials of the first kind &lt;em&gt;(&lt;/em&gt; \(P_{n} = \cos (n \arccos \theta)\) &lt;em&gt;)&lt;/em&gt;, which is \(w^{- \frac{1}{2} , -\frac{1}{2}}\). When using \(w^{- \frac{1}{2} , - \frac{1}{2}}\), &lt;em&gt;it can be proved that for differentiable target functions the approximation error is guaranteed to &lt;strong&gt;converge to zero&lt;/strong&gt;&lt;/em&gt;. I encourage you to research \(w^{0 , 0}\), which are the roots of the &lt;em&gt;Legendre polynomials&lt;/em&gt; and \(w^{\frac{1}{2} , \frac{1}{2}}\) which are the roots of the &lt;em&gt;Chebyshev polynomials of the second kind&lt;/em&gt;. These two node system &lt;em&gt;need enrichment to work optimally&lt;/em&gt;. It‚Äôs quite remarkable how Jacobi weights can describe a family of polynomials that contains so many notable specimens.&lt;/p&gt;

&lt;h3 id=&quot;node-systems-tested&quot;&gt;Node systems, tested&lt;/h3&gt;

&lt;p&gt;First of all we need to evaluate the roots of the Chebyshev polynomials: playing around with the definition will show that our roots are given by \(x_{i}=cos(\frac{\pi (i+\frac{1}{2})}{n})\) as \(i\) spans over \(0, \dots, n-1\). This is implemented in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chebyshevPolynomialRoots.m&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;          function xNodes = chebyshevPolynomialRoots( degree, targetIntervalLowerBound, targetIntervalUpperBound )
            xRaw = zeros( degree, 1 );    
            for i = 0 : degree - 1 
                    xRaw( i + 1 ) = cos( pi * ( i + 0.5) / degree );
            end
            xRaw = sort( xRaw );
            xNodes = 0.5 * xRaw ...
                        * ( targetIntervalUpperBound - targetIntervalLowerBound ) ...
                        + mean( [ targetIntervalLowerBound targetIntervalUpperBound ] );
          end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need a routine to evaluate our interpolating polynomial. Here it is, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lagrangePoly.m&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;          function y = lagrangePoly( nodes, components, x )
              piCoeff = poly( nodes );
              dPiCoeff = polyder( piCoeff );
              y = zeros( 1, length( x ) );
              for i = 1 : length( components )
                    y = y + ...
                        components(i) * ...
                        polyval( piCoeff, x ) ./ ...
                        ( polyval( dPiCoeff, nodes( i ) ) .* ( x - nodes( i ) ) );
              end
          end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This code allows us to evaluate the Lebesgue functions for different combinations of number of nodes and nodes system. First of all we will analyze the behavior of the Lebesgue functions in a system with evenly spaced nodes: note how &lt;strong&gt;the magnitude explodes near the edges of the interpolation interval&lt;/strong&gt;. This is tightly related to the Runge phenomenon. Think about the definition of the Lebesgue functions: &lt;em&gt;why is their value&lt;/em&gt; \(1\) &lt;em&gt;on the nodes?&lt;/em&gt;&lt;/p&gt;

&lt;figure style=&quot;display: inline-block; text-align: center; align-content:center; position: relative; float: left; overflow: hidden; margin: 0 20px 20px 0; max-width: 920px;&quot;&gt;
  &lt;img src=&quot;/assets/images/he-s-giving-the-numbers/lambda_e.gif&quot; alt=&quot;Lebesgue functions&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;position: absolute; text-align: center; align-content:center; background: rgba(0,0,0,0.75); color: white; padding: 10px 20px; opacity: 1; bottom: 0; font-size: 12px; line-height: 18px;&quot;&gt;Plots of the Lebesgue function with the number of nodes going from 1 to 10, evenly spaced nodes system. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This is how things change when we switch to the Chebyshev node system. The magnitude of the functions varies more evenly across the interval, yielding a better bound for the interpolation error.&lt;/p&gt;
&lt;figure style=&quot;display: inline-block; text-align: center; align-content:center; position: relative; float: left; overflow: hidden; margin: 0 20px 20px 0; max-width: 920px;&quot;&gt;
  &lt;img src=&quot;/assets/images/he-s-giving-the-numbers/lambda_cheby.gif&quot; alt=&quot;Lebesgue functions&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;position: absolute; text-align: center; align-content:center; background: rgba(0,0,0,0.75); color: white; padding: 10px 20px; opacity: 1; bottom: 0; font-size: 12px; line-height: 18px;&quot;&gt;Plots of the Lebesgue function with the number of nodes going from 1 to 10, Chebyshev nodes system. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Lastly, we take a look at the Runge phenomenon. When approximating \(\frac{1}{1+x^2}\) using algebraic interpolation on a system of 32 evenly spaced nodes we can see instabilities on the edges of the interval. This is related to the behavior of the Lebesgue functions that we saw in a previous figure. If we were to use the Chebyshev node system, the plot of the interpolated approximation and the one of the true function would be pretty much identical.&lt;/p&gt;
&lt;figure style=&quot;display: inline-block; text-align: center; align-content:center; position: relative; float: left; overflow: hidden; margin: 0 20px 20px 0; max-width: 920px;&quot;&gt;
  &lt;img src=&quot;/assets/images/he-s-giving-the-numbers/runge.jpg&quot; alt=&quot;Runge effect&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;position: absolute; text-align: center; align-content:center; background: rgba(0,0,0,0.75); color: white; padding: 10px 20px; opacity: 1; bottom: 0; font-size: 12px; line-height: 18px;&quot;&gt;Runge effect approximating the arctangent derivative using 32 evenly spaced nodes. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;what-about-splines&quot;&gt;What about splines?&lt;/h3&gt;

&lt;p&gt;Splines can be more advantageous as &lt;em&gt;they usually yield good results even when using very simple interpolating functions&lt;/em&gt;. Some of the most common splines that are used in practical applications are &lt;strong&gt;linear&lt;/strong&gt; and &lt;strong&gt;cubic&lt;/strong&gt; splines &lt;em&gt;(when dealing with functions from&lt;/em&gt; \(\mathbb{R}\) &lt;em&gt;to&lt;/em&gt; \(\mathbb{R}\) &lt;em&gt;)&lt;/em&gt; and &lt;strong&gt;Bezi√©r&lt;/strong&gt; splines &lt;em&gt;(when dealing with curves in a space or in a plane)&lt;/em&gt;. Usually one will find that &lt;em&gt;the approximation error with splines is a function of the distance between the nodes&lt;/em&gt; \(h\). Consider this example: &lt;em&gt;how can we bound the spline approximation error for a twice differentiable function over a system of evenly spaced nodes?&lt;/em&gt;
Let \(x_{i}\) and \(x_{i+1}\) be two consecutive nodes, and let \(x\) be a point in between these two nodes. Let \(s(x)=a_{s}x+b_{s}\) be the restriction of the spline to the interval \([ x_{i}, x_{i+1} ]\).
Let \(q\) be a constant, independent of \(x\).
Define the function \(g(y)\) to be the function such that&lt;/p&gt;

&lt;p&gt;\begin{equation}
  F(x)=f(x)-s(x)-q(x-x_{i})(x-x_{i+1})=0
\end{equation}&lt;/p&gt;

&lt;p&gt;\(F(x)\) has three distinct roots in \([ x_{i}, x_{i+1} ]\), therefore &lt;em&gt;Rolle‚Äôs theorem states that we have two roots&lt;/em&gt; for \(F&apos;(x)\). Again, use Rolle‚Äôs theorem to argue that \(F&apos;&apos;(x)\) &lt;em&gt;has a root&lt;/em&gt; \(\zeta\) &lt;em&gt;in&lt;/em&gt; \([ x_{i}, x_{i+1} ]\). \(s(x)\) is a piecewise linear function, therefore &lt;em&gt;its second derivative is null&lt;/em&gt;. We may state that&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \frac{d^{2}F}{dx^2}(\zeta)=\frac{d^{2}f}{dx^2}(\zeta) - 2q = 0
\end{equation}&lt;/p&gt;

&lt;p&gt;which means that \(q=\frac{1}{2}f&apos;&apos;(\zeta)\). Plug this in the definition of \(F(x)\) to state that&lt;/p&gt;

&lt;p&gt;\begin{equation}
  f(x)-s(x) = \frac{1}{2} \frac{d^{2}f}{dx^2} (\zeta)(x-x_{i})(x-x_{i+1})
\end{equation}&lt;/p&gt;

&lt;p&gt;and then assume the maximum over all possible values of \(x\) and all possible values of \(\zeta\) to obtain \(\|f(x)-s(x)\| \leq \frac{h^2}{8}M_{2}\), where \(M_{2} = \max _{[ x_{i}, x_{i+1} ]} \|f&apos;&apos;(x)\|\). You may think that this means that we should choose a number of nodes which is as high as possible. While this may work in theory, in practice &lt;em&gt;there is a threshold where machine arithmetic errors take over&lt;/em&gt; and deteriorate our results.&lt;/p&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;This first article briefly covered many concepts in algebraic interpolation. This is the foundation to then move forward and understand what numerical intergration is about. As usual, I‚Äôd like give you a list of possible topics to read more, but keep in mind that the world of numerical interpolation is quite extensive. Furthermore, we grazed some topics from other fields of mathematics, such as &lt;em&gt;orthogonal polynomial with respect to a weight&lt;/em&gt;. Some nice topics you may be interested in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;what if we change interpolating functions?&lt;/em&gt; A possible choice is to use sines and cosines: as &lt;em&gt;algebraic interpolation is a natural consequence of the existence of the Taylor series&lt;/em&gt; for a function, &lt;em&gt;armonic interpolation is a natural consequence of the existence of Fourier series&lt;/em&gt;;&lt;/li&gt;
  &lt;li&gt;efficency is key when we want to implement our numerical methods, so much so that you need to rethink even the basics such as evaluating polynomials. Check how &lt;em&gt;Horner‚Äôs method&lt;/em&gt; works and get yourself acquainted with big-O‚Äôs and other performance asymptotics;&lt;/li&gt;
  &lt;li&gt;weighted functional inner products are of fundamental importance in some fields of physics. Check how Laguerre polynomials &lt;em&gt;(orthogonal polynomials with respect to the exponential function)&lt;/em&gt; are used in modelling &lt;em&gt;hydrogen-like atoms&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As always, feel free to reach out to me on &lt;a href=&quot;https://linkedin.com/in/lbressan&quot;&gt;Linkedin&lt;/a&gt; or &lt;a href=&quot;https://github.com/luca-bressan&quot;&gt;GitHub&lt;/a&gt; if you want to suggest improvements or if you want to just have a chat about this. I hope that you enjoyed!&lt;/p&gt;

&lt;h3 id=&quot;bibliography-and-further-reading&quot;&gt;Bibliography and further reading&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/luca-bressan/blog-resources/blob/6e92793226fdab88d5801db6d2eb079ad6d45d14/approssimazione_algebrica_tna2020.pdf&quot;&gt;Approssimazione algebrica col metodo dei polinomi di Lagrange&lt;/a&gt; (in Italian). This is some work I did as an undergrad which served as a foundation for this article. It contains a lot of practical examples and in-depth considerations about the stuff we talked about;&lt;/li&gt;
  &lt;li&gt;Rodriguez, Giuseppe, 2008. &lt;em&gt;‚ÄúAlgoritmi Numerici‚Äù&lt;/em&gt; (in Italian);&lt;/li&gt;
  &lt;li&gt;Fermo, Luisa, 2018. &lt;em&gt;‚ÄúDispense per il corso di Teoria Numerica dell‚Äô Approssimazione‚Äù&lt;/em&gt; (in Italian);&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/w/index.php?title=Laguerre_polynomials&amp;amp;oldid=1127720816&quot;&gt;Laguerre polynomials.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Luca Bressan</name></author><category term="Computational-Finance" /><summary type="html">Cover photo by Waldemar Brandt on Unsplash. In Italy, when someone is speaking nonsense or is saying something outrageously wrong, we say that he‚Äôs ‚Äúgiving away the numbers‚Äù. Hopefully we‚Äôll be able to retain our mental fitness and say a lot of very correct stuff, while still giving away some numbers in a literal sense. What you should know before reading this post Basic linear algebra Basic calculus The quest for numerical integration There comes a point where our nice theoretical results, with closed formulas and all that jazz, are no longer sufficient to make a decision, give an estimate or complete our task. Sometimes we just need a number, a nicely written one using only digits, commas and some scientific notation for the adventurous. A particular class of problems that arises in obtaining our dear number is the one of numerical integration. Riemann and Lebesgue integrals are not particularly wieldy objects to begin with (that‚Äôs the reason why they‚Äôre plenty of fun, no?) and Ito integrals are even worse. In fact, for Ito intergrals, it‚Äôs quite hard to even state what a solution written with just digits is. Do we want a path-wise solution or do we just need some statistical properties to hold? We will try to get a grasp of all this tough stuff. But every nice journey begins with, well‚Ä¶ the beginning. This is part of a series of 3 articles: üôã‚Äç‚ôÇÔ∏è Numerical (algebraic) interpolation, the basis for numerical integration; Numerical approximation of Riemann integrals; Numerical approximation of Ito integrals, specifically targeting the issue of finding a weakly converging approximation scheme for processes built on the browninan motion (spoiler: this is what we need to price our beloved financial products). This time we are going to face the task of algebraic interpolation. While this article is going to be pretty far from being exhaustive (to be honest, there are 300 pages long books about interpolation which are pretty far from being exhaustive), it will serve as a starting point for our journey and I hope it will stimulate your curiosity. Well, enough preambles: 1, 2, 3, take a deep breath and‚Ä¶ let‚Äôs get going! What even is interpolation? The problem of interpolation arises when we want to find a function \(f(x)\) such that given a point cloud \((x_{i},y_{i})_{i = 1, 2, 3, \dots}\) we have \(f(x_{i})=y_{i}\). In particular, algebraic interpolation is the issue of finding \(f\) among the family of polynomials or piecewise polynomial functions. When the function is only piecewise polynomial, we are talking about spline interpolation. A case of particular interest is when there exists a known function \(\varphi(x)\) such that \(y = \varphi(x)\), where \(\varphi(x)\) has some unpleasant properties that make it difficult to evaluate. Another scenario where interpolation is useful is when \(\varphi(x)\) has some known properties but doesn‚Äôt allow for an explicit closed form. In this case we may still want to use a polynomial to approximate \(\varphi\), since polynomials are easy to handle and evaluate. There will be errors in doing this: we are going to measure them using the \(\infty\)-norm. Definition. (Approximation error) Given an interval \(I \subset \mathbb{R}\) and a function \(\varphi: I \to \mathbb{R}\) we define the approximation error obtained approximating \(\varphi\) over \(I\) using a polynomial \(f\), \(E(f;\varphi)_{I}\) as \begin{equation} E_{I}(f; \varphi) = \| f - \varphi \| _{ \infty } \restriction _{I} \end{equation} ‚àé This is our first glimpse of the essential nature of numerical analysis: knowing that you are wrong and by how much is even more important than the number you get at the end. Just as a note, sometimes you may mitigate this limitation of numerical analysis using tools like symbolic machine algebra. In theory, we can be quite successful at approximating functions by polynomials, we just need our functions to be continuous: Theorem. (Weierstrass Theorem on approximation errors) Let \(\varphi\) be a continuous function on some interval \(I\). Then for any \(\varepsilon &amp;gt; 0\) there exists \(n \in \mathbb{N}\) and a polynomial \(P_n\) of degree \(n\) such that \(E_{I}(P_n; \varphi) &amp;lt; \varepsilon\) ‚àé J-L. Lagrange in 1795 had the brilliant idea to define a vector space of polynomials with degree equal to the number of points in the cloud minus one. Lagrange polynomials have the nice property of having unit value on the various \(x_{i}\), and they constitute a basis for the linear space of polynomials, so that the interpolating polynomial is just a vector whose components are the \(y_{i}\). They look as follows: Definition. (Lagrange polynomials) Given a point cloud \((x_{i},y_{i})_{i = 1, 2, 3, \dots, n}\) we define the Lagrange basis of the \(n\)-degree polynomials space as \begin{equation} l_{k}=\frac{\pi_{n}(x)}{\pi_{n}‚Äô(x) (x-x_{k})} \end{equation} where \(\pi_{n}=(x-x_{1})(x-x_{2}) \dots (x-x_{n})\). ‚àé Evaluating the Lagrange interpolating polynomial \(L_{n}=y_{1}l_{1} + \dots + y_{n}l_{n}\) has computational complexity \(\mathcal{O}(n^{2})\), which is quite nice. Anything polynomial is actually nice, NP-hard problems are a massive issue. Where should I put my \(x_{i}\)? Choosing where to evaluate the ‚Äúhard‚Äù function before approximation plays a massive role in how good our interpolation will be. In fact we have this theorem: Theorem. (First Faber‚Äôs theorem) For any a priori known algorithm for determining the \(x_{i}\) independently of the target function \(\varphi\), there exist both a target function such that the approximation error does not converge to \(0\) as the number of \(x_{i}\) increases and one such that the approximation does indeed converge to \(0\) as \(n\) goes to infinity. ‚àé So, no matter how smart we are in pre-determining the \(x_{i}\) (from here on called nodes), sometimes we get things terribly wrong. But we can still be pretty smart. The following theorem tells us that we can bound the error by means of a recursive relation: Theorem. (Lebesgue constants theorem) Let \(\varphi\) be a continuous target function on some interval \(I\). Then the following relation holds \begin{equation} E_{I}(L_{n}; \varphi) \leq (1+\Lambda_{n}((x_{i})_{i = 1, \dots, n})) \mathcal{E}_{n-1}(\varphi) \end{equation} where \(\Lambda_{n} = \| \lambda_{n} \| _{\infty} \restriction_{I}\) are called Lebesgue‚Äôs constants, the \(n\)-th Lebesgue function \(\lambda_{n}\) is the sum of the absolute values of the Lagrange basis polynomials and \(\mathcal{E}(\varphi)\) is the best possible polynomial approximation of \(\varphi\) using polynomials of degree \(n\). ‚àé Given the recursive nature of this error bound, we would like to find an algorithm for selecting the nodes that allows us to have the Lebesgue constants grow as slowly as possible as we increase the number of nodes. There is, however, a limit to how slowly we can have them grow. Theorem. (Second Faber‚Äôs theorem) For any choice of node system, we have \(\Lambda_{n}((x_{i})_{i = 1, \dots, n}) \geq \frac{1}{12} \log (n)\) ‚àé Therefore, we have that if Lebesgue constants grow logarithmically, the node system has optimal scaling. Unsurprisingly, the smoothness of the target function also plays an important role. Theorem Let \(\varphi\) be a \(k\)-times differentiable target function on some interval \(I\). Then the following relation holds \begin{equation} E_{I}(L_{n}; \varphi) \leq c \frac{\log n}{n^k} \end{equation} where \(c\) is independent of \(k\) and \(n\) ‚àé This is it for the general facts. Let‚Äôs have a small recap befor moving on: interpolating basically means joining points in a nice way. And polynomials are really nice; Weierstrass‚Äô theorem makes it looks like algebraic interpolation can in theory be infinitely effective; Lagrange gave us a way of choosing the coefficients for our interpolating polynomial that just works; we can estimate errors; Faber‚Äôs theorems shattered our dreams of a universally good way of picking the interpolation nodes. Our main focus is the choice of the node system, let‚Äôs see some of those and let‚Äôs try to say something about their performance. Some notable node systems The first thing we would like to do is to use evenly spaced nodes across our interval. This seemingly reasonable choice is, unfortunately, an awful one. Theorem. (Tureckij theorem) For an evenly spaced system of nodes the Lebesgue constants have the following scaling: \begin{equation} \Lambda _ {n} ((x_{i})_{i = 1, \dots, n})) \sim \frac{2^{n}}{e n \log n} \end{equation} ‚àé This is related to an unpleasant phenomenon that we are going to see later, called the Runge effect. This leads many practitioners to limit the usage of evenly spaced nodes only with polynomials of degree at most equal to nine. We have smarter choices available, but first we need some extra notions. Definition. (Orthogonal polynomials) Two degree \(n\) polynomials \(p\) and \(q\) defined on the interval \([-1,1]\) are said to be orthogonal with respect to the \(\alpha , \beta\)-Jacobi weighted inner product if they satisfy the following condition \begin{equation} \int_{-1}^{1} p(x) q(x) (1-x)^{\alpha}(1+x)^{\beta} dx = 0 \end{equation} ‚àé Note that we can always traslate our approximation problem from a generic bounded interval into \([-1,1]\) by means of an affine transform. The roots of the orthogonal polynomial defined as before have several nice properties. We will refer to the node system given by these roots as \(w^{\alpha , \beta}\). Theorem. (Szeg≈ë theorem) If \(\alpha\) and \(\beta\) belong in \((-1,\frac{1}{2}]\), then the scaling of the Lebesgue constants of \(w^{\alpha , \beta}\) is logarithmic, and it is polynomial otherwise. ‚àé Szeg≈ë theorem therefore states how to reach that optimal logarithmic scaling of the error bound we were looking for. Sometimes, a process called enrichment is needed even when we are within the conditions stated by the theorem. We won‚Äôt talk about this process, but it basically is a way of adding nodes to improve performance. We will perform practical tests on two system of nodes of particular importance: the evenly spaced nodes (that we will indicate with \(\mathcal{E}\) ) and the the roots of the Chebyshev polynomials of the first kind ( \(P_{n} = \cos (n \arccos \theta)\) ), which is \(w^{- \frac{1}{2} , -\frac{1}{2}}\). When using \(w^{- \frac{1}{2} , - \frac{1}{2}}\), it can be proved that for differentiable target functions the approximation error is guaranteed to converge to zero. I encourage you to research \(w^{0 , 0}\), which are the roots of the Legendre polynomials and \(w^{\frac{1}{2} , \frac{1}{2}}\) which are the roots of the Chebyshev polynomials of the second kind. These two node system need enrichment to work optimally. It‚Äôs quite remarkable how Jacobi weights can describe a family of polynomials that contains so many notable specimens. Node systems, tested First of all we need to evaluate the roots of the Chebyshev polynomials: playing around with the definition will show that our roots are given by \(x_{i}=cos(\frac{\pi (i+\frac{1}{2})}{n})\) as \(i\) spans over \(0, \dots, n-1\). This is implemented in chebyshevPolynomialRoots.m: function xNodes = chebyshevPolynomialRoots( degree, targetIntervalLowerBound, targetIntervalUpperBound ) xRaw = zeros( degree, 1 ); for i = 0 : degree - 1 xRaw( i + 1 ) = cos( pi * ( i + 0.5) / degree ); end xRaw = sort( xRaw ); xNodes = 0.5 * xRaw ... * ( targetIntervalUpperBound - targetIntervalLowerBound ) ... + mean( [ targetIntervalLowerBound targetIntervalUpperBound ] ); end Now we need a routine to evaluate our interpolating polynomial. Here it is, lagrangePoly.m: function y = lagrangePoly( nodes, components, x ) piCoeff = poly( nodes ); dPiCoeff = polyder( piCoeff ); y = zeros( 1, length( x ) ); for i = 1 : length( components ) y = y + ... components(i) * ... polyval( piCoeff, x ) ./ ... ( polyval( dPiCoeff, nodes( i ) ) .* ( x - nodes( i ) ) ); end end This code allows us to evaluate the Lebesgue functions for different combinations of number of nodes and nodes system. First of all we will analyze the behavior of the Lebesgue functions in a system with evenly spaced nodes: note how the magnitude explodes near the edges of the interpolation interval. This is tightly related to the Runge phenomenon. Think about the definition of the Lebesgue functions: why is their value \(1\) on the nodes? Plots of the Lebesgue function with the number of nodes going from 1 to 10, evenly spaced nodes system. This is how things change when we switch to the Chebyshev node system. The magnitude of the functions varies more evenly across the interval, yielding a better bound for the interpolation error. Plots of the Lebesgue function with the number of nodes going from 1 to 10, Chebyshev nodes system. Lastly, we take a look at the Runge phenomenon. When approximating \(\frac{1}{1+x^2}\) using algebraic interpolation on a system of 32 evenly spaced nodes we can see instabilities on the edges of the interval. This is related to the behavior of the Lebesgue functions that we saw in a previous figure. If we were to use the Chebyshev node system, the plot of the interpolated approximation and the one of the true function would be pretty much identical. Runge effect approximating the arctangent derivative using 32 evenly spaced nodes. What about splines? Splines can be more advantageous as they usually yield good results even when using very simple interpolating functions. Some of the most common splines that are used in practical applications are linear and cubic splines (when dealing with functions from \(\mathbb{R}\) to \(\mathbb{R}\) ) and Bezi√©r splines (when dealing with curves in a space or in a plane). Usually one will find that the approximation error with splines is a function of the distance between the nodes \(h\). Consider this example: how can we bound the spline approximation error for a twice differentiable function over a system of evenly spaced nodes? Let \(x_{i}\) and \(x_{i+1}\) be two consecutive nodes, and let \(x\) be a point in between these two nodes. Let \(s(x)=a_{s}x+b_{s}\) be the restriction of the spline to the interval \([ x_{i}, x_{i+1} ]\). Let \(q\) be a constant, independent of \(x\). Define the function \(g(y)\) to be the function such that \begin{equation} F(x)=f(x)-s(x)-q(x-x_{i})(x-x_{i+1})=0 \end{equation} \(F(x)\) has three distinct roots in \([ x_{i}, x_{i+1} ]\), therefore Rolle‚Äôs theorem states that we have two roots for \(F&apos;(x)\). Again, use Rolle‚Äôs theorem to argue that \(F&apos;&apos;(x)\) has a root \(\zeta\) in \([ x_{i}, x_{i+1} ]\). \(s(x)\) is a piecewise linear function, therefore its second derivative is null. We may state that \begin{equation} \frac{d^{2}F}{dx^2}(\zeta)=\frac{d^{2}f}{dx^2}(\zeta) - 2q = 0 \end{equation} which means that \(q=\frac{1}{2}f&apos;&apos;(\zeta)\). Plug this in the definition of \(F(x)\) to state that \begin{equation} f(x)-s(x) = \frac{1}{2} \frac{d^{2}f}{dx^2} (\zeta)(x-x_{i})(x-x_{i+1}) \end{equation} and then assume the maximum over all possible values of \(x\) and all possible values of \(\zeta\) to obtain \(\|f(x)-s(x)\| \leq \frac{h^2}{8}M_{2}\), where \(M_{2} = \max _{[ x_{i}, x_{i+1} ]} \|f&apos;&apos;(x)\|\). You may think that this means that we should choose a number of nodes which is as high as possible. While this may work in theory, in practice there is a threshold where machine arithmetic errors take over and deteriorate our results. Conclusions This first article briefly covered many concepts in algebraic interpolation. This is the foundation to then move forward and understand what numerical intergration is about. As usual, I‚Äôd like give you a list of possible topics to read more, but keep in mind that the world of numerical interpolation is quite extensive. Furthermore, we grazed some topics from other fields of mathematics, such as orthogonal polynomial with respect to a weight. Some nice topics you may be interested in: what if we change interpolating functions? A possible choice is to use sines and cosines: as algebraic interpolation is a natural consequence of the existence of the Taylor series for a function, armonic interpolation is a natural consequence of the existence of Fourier series; efficency is key when we want to implement our numerical methods, so much so that you need to rethink even the basics such as evaluating polynomials. Check how Horner‚Äôs method works and get yourself acquainted with big-O‚Äôs and other performance asymptotics; weighted functional inner products are of fundamental importance in some fields of physics. Check how Laguerre polynomials (orthogonal polynomials with respect to the exponential function) are used in modelling hydrogen-like atoms. As always, feel free to reach out to me on Linkedin or GitHub if you want to suggest improvements or if you want to just have a chat about this. I hope that you enjoyed! Bibliography and further reading Approssimazione algebrica col metodo dei polinomi di Lagrange (in Italian). This is some work I did as an undergrad which served as a foundation for this article. It contains a lot of practical examples and in-depth considerations about the stuff we talked about; Rodriguez, Giuseppe, 2008. ‚ÄúAlgoritmi Numerici‚Äù (in Italian); Fermo, Luisa, 2018. ‚ÄúDispense per il corso di Teoria Numerica dell‚Äô Approssimazione‚Äù (in Italian); Laguerre polynomials.</summary></entry><entry><title type="html">Some facts about barrier options</title><link href="http://localhost:4000/about-barrier-options" rel="alternate" type="text/html" title="Some facts about barrier options" /><published>2022-10-10T12:18:00+02:00</published><updated>2022-10-10T12:18:00+02:00</updated><id>http://localhost:4000/about-barrier-options</id><content type="html" xml:base="http://localhost:4000/about-barrier-options">&lt;p&gt;Cover photo by &lt;a href=&quot;https://unsplash.com/photos/plbeyaME7Jk?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditShareLink&quot;&gt;T L&lt;/a&gt;. Arguably, a dam is a down-and-out type of barrier, although not the kind we‚Äôre interested in.&lt;/p&gt;

&lt;h3 id=&quot;what-you-should-know-before-reading-this-post&quot;&gt;What you should know before reading this post&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Fundamentals of option pricing in the &lt;em&gt;Black-Scholes&lt;/em&gt; model.&lt;/li&gt;
  &lt;li&gt;Fundamentals of stochastic calculus.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;about-barrier-options-and-other-related-contracts&quot;&gt;About barrier options and other related contracts&lt;/h1&gt;

&lt;p&gt;Let‚Äôs be honest: option pricing is not an easy feat to begin with, and the work of Black and Scholes is nothing short of magic. Today we are going to face the issue of options with path-dependent payoffs, in particular we are going to take a look at single, fixed barrier options and lookbacks. We will introduce some mathematical results that allow us to think more &lt;em&gt;financially&lt;/em&gt; when dealing with the pricing problem, instead of getting bogged down in the maths of it. We will test some of our results with a Monte Carlo estimate, checking whether the closed form formula agrees with simulations. I suggest that you read this on a decently sized screen: MathJax is notoriously mobile-unfriendly. Rotating your device in landscape mode may help with long formulas and equations. Take a deep breath, it‚Äôs going to be kind of intenseüòÇ.&lt;/p&gt;

&lt;p&gt;Let‚Äôs begin!&lt;/p&gt;

&lt;h3 id=&quot;a-closer-look-at-the-black-scholes-equation&quot;&gt;A closer look at the &lt;em&gt;Black-Scholes&lt;/em&gt; equation&lt;/h3&gt;

&lt;p&gt;It‚Äôs easy to lose sight of the target when dealing with an option pricing problem. That is, many forget that the option pricing problem &lt;em&gt;is &lt;strong&gt;not&lt;/strong&gt; about find the general integral of the parabolic PDE that is the Black-Scholes equation&lt;/em&gt;, but it &lt;em&gt;is about finding &lt;strong&gt;the unique solution to a boundary constraint problem&lt;/strong&gt;&lt;/em&gt;. Why does this matter so much?
Let‚Äôs introduce the &lt;strong&gt;Black-Scholes PDE operator&lt;/strong&gt;: for each suitable function \(f\), let \(\mathfrak{d}_{\sigma,r}(f)\) be the operator that returns the function&lt;/p&gt;

&lt;p&gt;\begin{equation}
              \mathfrak{d}_{\sigma,r}(f) := \frac{\partial f}{\partial t} + \frac{1}{2} \sigma^2 x^2 \frac{\partial^2 f}{\partial x^2} + rx \frac{\partial f}{\partial x} - rf
\end{equation}&lt;/p&gt;

&lt;p&gt;One might see that \(\mathfrak{d}_{\sigma,r}(f)\) is a linear operator. This is of fundamental importance to our goal: two different solutions to our &lt;em&gt;PDE&lt;/em&gt; can be combined to get a new one. Should we be able to combine different solutions to the equation to satisfy the boundary condition, we would be successful in solving the option pricing problem. Let‚Äôs introduce the crucial tools that will allow us to exploit this property.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;proposition&quot;&gt;Proposition&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let
\begin{equation}
\frac{dX}{X} = \mu dt + \sigma dW_{t}
\end{equation}
be the dynamic of a geometric brownian motion under a suitable probability measure. Then there exists a number \(\alpha\) such that \(X^{\alpha}\) is a martingale under the same probability measure.&lt;/p&gt;

&lt;h6 id=&quot;proof-sketch&quot;&gt;Proof sketch&lt;/h6&gt;

&lt;p&gt;The appropriate \(\alpha\) is \(\alpha = 1 - \frac{2 \mu}{\sigma^2}\). From here one should use Ito‚Äôs formula on \(X^{\alpha}\) and see that the resulting dynamic is one of a martingale.&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;This result is going to be crucial in the next proof. We still need some preliminary work before diving into barrier option pricing.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;definition-contract-function&quot;&gt;Definition. &lt;em&gt;(Contract function)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;A &lt;em&gt;contract function&lt;/em&gt; \(\Phi_{A_{T}}\) on some asset \(A_{t}\) is a generic function of at least asset price and time of expiration. For our intents and purposes, it will represent exactly the value of a given contract at expiration.&lt;/p&gt;

&lt;p&gt;Our contract functions unfortunately won‚Äôt be just function of asset price and time of expiration. They will be functions of the asset path as well, that is they will be functions of \(A_{t}\) for some \(t&amp;lt;T\). This is what we mean when we talk about path-dependency.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;definition-pricing-function-price-function&quot;&gt;Definition. &lt;em&gt;(Pricing function, price function)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;A &lt;em&gt;pricing function&lt;/em&gt; \(\pi_{\Phi_{A_{T}}, t}\) on some contract \(\Phi_{A_{T}}\) is a function that gives the no-arbitrage price of said contract at any time before expiration.&lt;/p&gt;

&lt;p&gt;Sometimes I won‚Äôt be as pedantic when using notation. No need to be as long as we‚Äôre understating each other. Be flexible!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h6 id=&quot;theorem-principle-of-reflection&quot;&gt;Theorem. &lt;em&gt;(Principle of Reflection)&lt;/em&gt;&lt;/h6&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let \(\Phi_{S_{T}}\) be a contract function on some asset \(S_{t}\) that follows a GBM dynamic in a complete market free of arbitrages, and let \(\pi_{\Phi_{S_{T}}, t}\) be the associated pricing function that solves the &lt;em&gt;Black-Scholes PDE&lt;/em&gt; \(\mathfrak{d}_{\sigma,r} ( \pi_{\Phi_{S_{T}}, t} ) = 0\).
Let \(H&amp;gt;0\) and let \(\alpha\) be the number such that \(S^{\alpha}\) is a martingale under the same probability measure. Consider the contract function \(\Psi_{S_{T}}=\left( \frac{S_{T}}{H} \right) ^{\alpha} \Phi_{\frac{H^2}{S_{T}}, t}\)
Then, the associated pricing function to the new claim that solves the &lt;em&gt;Black-Scholes PDE&lt;/em&gt; is
\begin{equation}
\xi_{S_{t},t} = \left( \frac{S_{t}}{H} \right) ^{\alpha} \pi_{\Phi_{S_{T}}, t} \left( \frac{H^2}{S_{T}}, t \right) .
\end{equation}&lt;/p&gt;

&lt;h6 id=&quot;proof&quot;&gt;Proof&lt;/h6&gt;

&lt;p&gt;The appropriate price (that is, the pricing function that solves the Black-Scholes PDE) for \(\Psi_{S_{T}}\) given by&lt;/p&gt;

&lt;p&gt;\begin{equation}
\xi=e^{-r(T-t)} \mathbb{E}^{\mathbb{Q},t} \left[ \left( \frac{S_{t}}{H} \right) ^{\alpha} \Phi_{\frac{H^2}{S_{T}}, t} \right]
\end{equation}
where \(\mathbb{Q}\) is the probability measure such that \(S_{t}^{\alpha}\) is a martingale.&lt;/p&gt;

&lt;p&gt;Consider \(\Psi_{S_{T}}\) to be a contract on asset \(U_{t}=\frac{H^2}{S_{t}}\).
Then, use the abstract Bayes theorem to state that there exists a new probability measure \(\mathbb{B}\), obtainable via a Radon-Nikodym derivative, such that&lt;/p&gt;

&lt;p&gt;\begin{equation}
\xi \left( \frac{S_{t}}{H} \right) ^{- \alpha} = e^{-r(T-t)} \mathbb{E}^{\mathbb{B},t} \left[ \Phi_{U_{T},T} \right]
\end{equation}&lt;/p&gt;

&lt;p&gt;Furthermore, the following can be stated:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(dW^{\mathbb{B}}=dW^{\mathbb{Q}}-\alpha \sigma dt\).&lt;/li&gt;
  &lt;li&gt;\(U\) has \(\mathbb{B}\)-dynamic \(\frac{dU}{U} = \mu dt - \sigma dW^{\mathbb{B}}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since the \(\mathbb{B}\)-dynamic of \(U_{t}\) and the \(\mathbb{Q}\)-dynamic of \(S_{t}\) are equivalent &lt;strong&gt;as the minus sign in front of the brownian differential does not impact the distribution&lt;/strong&gt;, we may state that the right-hand side of the pricing equation is exactly \(\pi\) evaluated for the desired arguments. Multiply then both sides by \(\left( \frac{S_{t}}{H} \right) ^{\alpha}\) to conclude the proof.&lt;/p&gt;

&lt;p style=&quot;text-align: right&quot;&gt;‚àé&lt;/p&gt;

&lt;p&gt;Let‚Äôs add some remarks on this proof. Some of the readers might be familiar with other names for the abstract Bayes‚Äô theorem: for example it is also called &lt;em&gt;change of numerary theorem&lt;/em&gt;, and those who are familiar with the Black‚Äôs formula for futures pricing are also aware of its immense versatility. It can be used to prove other pricing formulas, like Margrabe‚Äôs formula for exchange options. Furthermore, we are assuming constant volatility, constant risk-free rate and no dividend yield. While accounting for deterministically varying dividends, rates and volatility is relatively easy (see &lt;a href=&quot;#bibliography-and-further-reading&quot;&gt;Bj√∂rk&lt;/a&gt;), accounting for random variations in this parameters is much more tedious and may preclude the possibility of finding a closed form for the prices (see &lt;a href=&quot;#bibliography-and-further-reading&quot;&gt;Google Scholar: pricing options under the Vasicek interest rate model&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Let‚Äôs move on: why did we bother with the reflection theorem in the first place? The assets \(S_{t}\) and \(U_{t}\) have a peculiar property: they are always on opposite sides of the number \(H\), and they match when \(S_{t}=H\). By this point you may have already correctly guessed that \(H\) is our barrier. But this is where the real magic happens: consider a vanilla call option struck at \(K\). The reflection principle allows us to synthesize a new contract &lt;em&gt;(from now on called reflected)&lt;/em&gt; that has the exact same price of the original one when \(S_{t}=H\), independently of time. This is the key point of what we‚Äôre about to do.&lt;/p&gt;

&lt;h3 id=&quot;pricing-a-down-and-out-regular-call-option&quot;&gt;Pricing a down-and-out regular call option&lt;/h3&gt;

&lt;p&gt;Consider a down-and-out call option struck at \(K\) where the barrier \(H\) is out-of-the-money &lt;em&gt;(this last condition is what distinguishes regular barrier options from reverse barrier options)&lt;/em&gt;. The pricing problem is a boundary constrained problem, built on the &lt;em&gt;Black-Scholes PDE&lt;/em&gt;. The constraints are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;terminal pricing condition: the price at expiry must match the payoff of the contract;&lt;/li&gt;
  &lt;li&gt;lack-of-optionality condition: when the asset price is very high, our option must be treated more and more like a futures contract, since the benefit they provide is equivalent;&lt;/li&gt;
  &lt;li&gt;barrier condition: whenever the underlying asset touches the barrier, the option ceases to exist, turning into nothingness. And nothing has no value.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We claim that a portfolio made by going long on a vanilla call with strike \(K\) and going short on the reflection of such call option is going to provide the solution to our pricing problem:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;at t=T, our reflected call is always going to be out-of-the-money, as the barrier was not touched. One can see that by observing that \(\frac{H^2}{S_{T}} &amp;lt; \frac{H^2}{H} = H &amp;lt; K\). This means that the portfolio value is entirely given by our vanilla call, as intended;&lt;/li&gt;
  &lt;li&gt;as \(S_{t}\) goes to infinity, so \(\frac{H^2}{S_{T}}\) goes to zero, meaning the the reflected call has negligible value and our portfolio inherits its lack-of-optionality from the vanilla call;&lt;/li&gt;
  &lt;li&gt;independently of time, whenever \(S_{t}\) equals \(H\) the reflected call value cancels out the vanilla call value, giving a net value to the portfolio equal to zero.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An important consideration must be added: our process works under the assumption that our contract function has an indicator function to nullify its value whenever the barrier is touched. In symbols, we are assuming our contract function to look like this \(\xi_{(...)} = \tilde{\xi}_{(...)} \mathbb{1}_{\lbrace \forall t : S_{t}&amp;gt;H \rbrace}\): this is what allows us not to worry about what happens when \(S_{t}\) is below the barrier.&lt;/p&gt;

&lt;p&gt;We can now state what the price for this barrier option is:&lt;/p&gt;

&lt;p&gt;\begin{equation}
C^{DOC,regular} (S_{t}, t, \sigma, r, H, K) = C^{vanilla} \left( S_{t}, t, \sigma, r, K \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) - \left( \frac{S_{t}}{H} \right)^{\alpha} C^{vanilla} \left( \frac{H^2}{S_{t}}, t, \sigma, r, K \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t})
\end{equation}&lt;/p&gt;

&lt;p&gt;Unsurprisingly, we observe that the barrier clause generates a discount on the final price.&lt;/p&gt;

&lt;figure style=&quot;display: inline-block; text-align: center; align-content:center; position: relative; float: left; overflow: hidden; margin: 0 20px 20px 0; max-width: 920px;&quot;&gt;
  &lt;img src=&quot;/assets/images/about-barrier-options/price_animation.gif&quot; alt=&quot;Price evolution&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;position: absolute; text-align: center; align-content:center; background: rgba(0,0,0,0.75); color: white; padding: 10px 20px; opacity: 1; bottom: 0; font-size: 12px; line-height: 18px;&quot;&gt;Evolution of price functions for a plain vanilla call (in yellow) and a DOC regular option (in blue). Notice how the blue line always lies below the yellow line. In red, the payoff at maturity.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;pricing-a-down-and-out-reverse-call&quot;&gt;Pricing a down-and-out reverse call&lt;/h3&gt;

&lt;p&gt;When the barrier is in-the-money, our barrier condition is no longer satisfied if we just use the previous approach. We need to make some minor alterations to the options portfolio we intend to reflect. We should build a portfolio of derivatives whose value is zero at the barrier, while having exactly the value of a vanilla call option whenever the underlying asset price is higher. This is obtained by going long on one vanilla call struck at \(H\) and \(H-K\) units of a digital call also struck at \(H\). Reflect this portfolio to obtain the following pricing formula:&lt;/p&gt;

&lt;p&gt;\begin{equation}
C^{DOC,reverse} (S_{t}, t, \sigma, r, H, K) = C^{vanilla} \left( S_{t}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) + (H-K) C^{digital} \left( S_{t}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) - \left( \frac{S_{t}}{H} \right)^{\alpha} C^{vanilla} \left( \frac{H^2}{S_{t}}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) - (H-K) \left( \frac{S_{t}}{H} \right)^{\alpha} C^{digital} \left( \frac{H^2}{S_{t}}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t})
\end{equation}&lt;/p&gt;

&lt;figure style=&quot;display: inline-block; text-align: center; align-content:center; position: relative; float: left; overflow: hidden; margin: 0 20px 20px 0; max-width: 920px;&quot;&gt;
  &lt;img src=&quot;/assets/images/about-barrier-options/payoff_truncation.jpg&quot; alt=&quot;reflected portfolio&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;position: absolute; text-align: center; align-content:center; background: rgba(0,0,0,0.75); color: white; padding: 10px 20px; opacity: 1; bottom: 0; font-size: 12px; line-height: 18px;&quot;&gt;Maturity payoff of the truncated portfolio. Solid red line indicates the contract function, dashed black line indicates the barrier threshold and dot-dashed blue line indicates the strike price of the option. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;in-out-parities&quot;&gt;In-Out parities&lt;/h3&gt;

&lt;p&gt;Whenever a portfolio is made up of knock-out barrier option and the respective knock-in counterpart, the resulting portfolio is a plain vanilla option of the same type. While this is intuitively clear, a quick algebraic manipulation shows that this fact is also mathematically correct. Feel free to have a try proving this, it‚Äôs easy. It is just a matter of being careful with indicator functions algebra. This allows us to price the knock-in contracts given the knock-out price and viceversa.&lt;/p&gt;

&lt;h3 id=&quot;pricing-an-up-and-out-regular-put-and-pretty-much-anything-else&quot;&gt;Pricing an up-and-out regular put, and pretty much anything else&lt;/h3&gt;

&lt;p&gt;The reflection principle is still our primary tool when it comes to pricing up contracts. Our approach is still valid even with our new boundary constraints. Namely, our &lt;em&gt;spatial constraints&lt;/em&gt; define a bounded region in the set of possible \(S_{t}\) values (the term &lt;em&gt;spatial constraint&lt;/em&gt; comes from &lt;a href=&quot;#conclusions&quot;&gt;thermodynamics&lt;/a&gt;). What we did by imposing the knock-out clause is essentialy setting an upper bound to the values that \(S_{t}\) can take without nullifying our contract. A lower bound was already present, since in our model asset prices cannot be negative. It is left to the reader to try out every possibile combination of put, call, knock-in, knock-out regular and reverse contract. This approach applies to pretty much every contract function as long as the barrier is unique and fixed. Let‚Äôs close this discussion with a quick question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There are barrier contracts which are exactly equivalent to their vanilla counterparts, which ones? &lt;em&gt;Think of barrier contracts which have zero value no matter the time to expiration or the value of the underlying&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;testing-our-formulas&quot;&gt;Testing our formulas&lt;/h3&gt;

&lt;p&gt;It would be rather useful to check whether our previous work is correct by comparing it with something that is certainly correct. We shall do this by comparing the result of our formulas with the result of a Monte Carlo estimation. Our simulation will be done using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MATLAB&lt;/code&gt;.
Consider an hypotethical asset-market combination with the following properties: \(S_{0}=100 \$\), \(r=2.5 \%\), \(\sigma = 20\%\). We will try pricing a down-and-out regular call with \(H=95\), \(T=1\) and \(K=102.5 \$\). It is essentially an ATMF call option with a knock-out clause. We will simulate 30000 paths: unfortunately Monte Carlo methods are slow to converge. Let‚Äôs take a look at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MATLAB&lt;/code&gt; code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;          % Pricing parameters
          mu = 0.025;
          sigma = 0.2;
          s_0 = 100;
          deltaTime=1/(252*8*12);
          numDataPoints=252*8*12;
          numPaths=30000;
          K=102.5;
          H=95;
          alpha=1-2*(mu/(sigma^2));

          % Asset paths simulation
          asset = gbm(mu,sigma,&apos;StartState&apos;,s_0);
          tic;
          [X, T] = asset.simByEuler(numDataPOints, &apos;DeltaTime&apos;, deltaTime, &apos;nTrials&apos;, numPaths);
          toc
          assetPaths=zeros(numDataPoints+1,numPaths);
          for i=1:numPaths
                        assetPaths(:,i)=X(:,:,i);
          end

          % Option price estimation
          optionSimulatedPayoffs=zeros(1,numPaths);
          for k=1:numPaths
                        if min(assetPaths(:,k))&amp;lt;=H
                                      optionSimulatedPayoffs(k)=0;
                        else
                                      optionSimulatedPayoffs=max(assetPaths(end,k)-K, 0);
                        end
          end

          optionEstimatedPrice=exp(-mu)*mean(optionSimulatedPayoffs);
          optionExactPrice=blsprice(s_0,K,mu,1,sigma) - ((s_0/H)^alpha).*blsprice(H^2/s_0,K,mu,1,sigma);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code is very much self explainatory, and for details about stochastic processes simulation one should refer to the official documentation by MathWorks. One might wonder why the number of data points is such that we are considering observations taken every quarter of an hour over a day (or taken every five minutes over an 8-hours working day). If we were to consider observations taken at the end of each trading day, we would be giving an overly optimistic and biased estimate of the option price as &lt;a href=&quot;#bibliography-and-further-reading&quot;&gt;we would be discarding effects due to intraday volatility&lt;/a&gt;. To make a long story short, our asset may breach the barrier in between observation points, and we wouldn‚Äôt be able to notice that.
Our estimated price is \(V=4.39(8)\$\). Assuming an approximately normal distribution for the estimator &lt;em&gt;(can you spot any issues with this?)&lt;/em&gt; we get a standard error of \(0.06(8) \$\) and our theoretical price of \(4.34(5) \$\) is within the 95% confidence interval of our estimation. We can conclude that our Monte-Carlo estimate and our closed-form analytical formula agree on the result to a sufficient degree that we can confidently assign a price to the option.&lt;/p&gt;

&lt;figure style=&quot;display: inline-block; text-align: center; align-content:center; position: relative; float: left; overflow: hidden; margin: 0 20px 20px 0; max-width: 920px;&quot;&gt;
  &lt;img src=&quot;/assets/images/about-barrier-options/montecarlo_paths.jpg&quot; alt=&quot;Monte Carlo paths&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;position: absolute; text-align: center; align-content:center; background: rgba(0,0,0,0.75); color: white; padding: 10px 20px; opacity: 1; bottom: 0; font-size: 12px; line-height: 18px;&quot;&gt;A sampling of 100 simulated asset paths. The red line marks the average of the simulated asset paths. In blue it is shown the risk-free growth of a capital equal to the spot price of the underlying. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;static-hedging-of-barrier-options&quot;&gt;Static hedging of barrier options&lt;/h3&gt;

&lt;p&gt;Once the appropriate formula for a price is found, it is natural to ask ourselves how to hedge a position involving barrier options. Unfortunately, when taking derivatives to evaluate \(\Delta\), the indicator functions induce finite jump discontinuities in the result. This means that hedging a position when the underlying is near the barrier can be extremely problematic, and static hedging techniques are out of the scope of this article. In the last 20 years the literature about solving this issue has been very abundant, so a &lt;a href=&quot;#bibliography-and-further-reading&quot;&gt;search on Google Scholar&lt;/a&gt; can be a good starting point for learning more.&lt;/p&gt;
&lt;figure style=&quot;display: inline-block; text-align: center; align-content:center; position: relative; float: left; overflow: hidden; margin: 0 20px 20px 0; max-width: 920px;&quot;&gt;
  &lt;img src=&quot;/assets/images/about-barrier-options/dic.jpg&quot; alt=&quot;DIC&quot; style=&quot;max-width: 100%; height: auto;&quot; /&gt;
  &lt;figcaption style=&quot;position: absolute; text-align: center; align-content:center; background: rgba(0,0,0,0.75); color: white; padding: 10px 20px; opacity: 1; bottom: 0; font-size: 12px; line-height: 18px;&quot;&gt;Price and delta as a function of the underlying asset price for some DIC option. Notice the cusp in the price function and discontinuity at the barrier in the delta.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;getting-creative-pricing-a-floating-strike-lookback-call&quot;&gt;Getting creative: pricing a floating strike lookback call&lt;/h3&gt;

&lt;p&gt;At the heart of our work lies the issue of path-dependency. That is, our contract function is not only a function of information known at the time of expiration, but it also depends on other times preceding the expiration. Our clever solution with reflections had the only goal of removing path-dependency from our calculations. Now we will take a look at another contract that exhibits path dependency. Floating strike lookback calls are options that allow us to buy an asset at the lowest price recorded in between issuance of the option and expiration. Our contract function looks like this:&lt;/p&gt;

&lt;p&gt;\begin{equation}
\Phi = S_{T} - \min_{t= \tau} S_{\tau}
\end{equation}&lt;/p&gt;

&lt;p&gt;We will obtain a pricing formula decomposing this contract in a portfolio of options that are either not path-dependent or whose path-dependency we can manage. Let \(t\) be some time before expiration. Let \(\tau(t)\) be the time where our asset reached it lowest price, that is let \(\tau(t) = \arg \min _{t} S_{t}\). Intuitively, our portfolio must start with a plain vanilla call option, struck at \(S_{\tau(t)}\), given that surely we‚Äôll have the right to buy the asset at such price. Then, our portfolio must take into account the possibility that the asset will fall even lower than \(S_{\tau(t)}\). We will add something to our portfolio that satisfies the following requirements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;we must be able to price it using reflections;&lt;/li&gt;
  &lt;li&gt;no benefit must be provided at expiration if \(S_{\tau(t)} = S_{\tau(T)}\);&lt;/li&gt;
  &lt;li&gt;the resulting portfolio must be a call struck at \(S_{\tau(T)}\) (i.e. our lookback option).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We claim that this result can be achieved by adding infinitely many contracts, each one of them being bought in infinitesimal quantity. Consider for any \(\gamma\) lesser than \(S_{\tau(t)}\), a position obtained by going short \(d\gamma\) units of a down-and-in digital call struck at \(S_{\tau(t)}\) with barrier at \(\gamma\) and going long by the same amount on a down and in digital call struck at \(\gamma\) with barrier at \(\gamma\). Let‚Äôs check our claim:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;we can price digital DICs using reflections;&lt;/li&gt;
  &lt;li&gt;since any \(\gamma\) is lesser than \(S_{\tau(t)}\), if \(S_{\tau(t)} = S_{\tau(T)}\) none of our contracts are active, and the payoff is null.&lt;/li&gt;
  &lt;li&gt;suppose that \(S_{T}\) is greater than \(S_{\tau(t)}\). Integrate the payoff over the active, non zero value contract region (i.e. between \(S_{\tau(t)}\) and \(S_{\tau(T)}\)): \(\int_{S_{\tau(T)}}^{S_{\tau(t)}} d\gamma = S_{\tau(t)} - S_{\tau(T)}\). Then add to our previous result the payoff of our plain vanilla call: it is \(S_{T}-S_{\tau(t)}\), and therefore we obtain the right quantity. If \(S_{T}\) is lesser than \(S_{\tau(t)}\), the above integral reduces to \(\int_{S_{\tau(T)}}^{S_{T}} d\gamma = S_{T} - S_{\tau(T)}\), and adding the null payoff of the plain vanilla call yields again the correct result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of this allows us to state that the the price for a lookback floating strike call is given by&lt;/p&gt;

&lt;p&gt;\begin{equation}
\int_0^{S_{\tau(t)}} \left( C^{DIC,digital}(S_{t}, t, \sigma, r, \gamma, \gamma) - C^{DIC,digital}(S_{t}, t, \sigma, r, \gamma, S_{\tau(t)} \right) d\gamma + C^{call}(S_{t}, t, \sigma, r, S_{\tau(t)})
\end{equation}&lt;/p&gt;

&lt;p&gt;You‚Äôll be delighted to know that this integral allows for a closed form analytical solution, but solving the integral requires a certain bravery, a bit of time and a ton of paper. If you‚Äôre up for a nice integration challenge, go for it. You will find out that the solution is&lt;/p&gt;

&lt;p&gt;\begin{equation}
S_{t}e^{-r(T-t)}N(d_{1})-S_{\tau(t)}e^{-r(T-t)}N(d_{2})+S_{t}e^{-r(T-t)} \sigma \sqrt{T-t} \left[ \frac{1}{\sqrt{2\pi}}  e^{- \frac{d_{1}^2}{2}}+d_{1}N(-d_{1}) \right]
\end{equation}&lt;/p&gt;

&lt;p&gt;where \(N(x)\) is the standard gaussian cdf. and \(d_{1}\) and \(d_{2}\) are the usual &lt;em&gt;Black-Scholes quantiles&lt;/em&gt; evaluated for \(K=S_{\tau(t)}\) and \(S=S_{t}\). This expression is somewhat more complex if we allow for the presence of dividends, but the reasoning remains the same. Again feel free to try, I‚Äôm sure you‚Äôll manage as it‚Äôs mostly a matter of making the right choices while integrating by parts.&lt;/p&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;We have seen how the reflection principle allows us to get rid of path-dependency and lets us find pricing formula that would be otherwise much more tedious to evaluate. This is not the only way possible: you may want to look into the theory of absorbed processes. This is the way that these formula were first found (see &lt;a href=&quot;#bibliography-and-further-reading&quot;&gt;Muisela&lt;/a&gt; and &lt;a href=&quot;#bibliography-and-further-reading&quot;&gt;Bj√∂rk&lt;/a&gt;), and it is a perfecty valid one, although much more mathematically intense. The beauty of the reflection principle lies in the possibilty of being able to reason in &lt;em&gt;financial terms&lt;/em&gt;, and the math just gets out of the way. This article is a good starting point, but much more can be said about path-dependent contract functions. Some possible next steps from here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;looking into more contracts, such as ladders and double barriers;&lt;/li&gt;
  &lt;li&gt;looking at how things change when we have stochastic dividends and rates;&lt;/li&gt;
  &lt;li&gt;looking at the classic heat diffusion model &lt;em&gt;(interestingly, the option pricing problem is very similar to the problem of predicting temperature evolution across an infinitely long rod)&lt;/em&gt;, and then looking at the heat diffusion model with constant temperature zones &lt;em&gt;(you will find that they behave very much like our barriers)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is my first article for this blog, and my first attempt at blogging in general. Reach out to me on &lt;a href=&quot;https://linkedin.com/in/lbressan&quot;&gt;Linkedin&lt;/a&gt; or &lt;a href=&quot;https://github.com/luca-bressan&quot;&gt;GitHub&lt;/a&gt; if you want to suggest improvements or if you want to just have a chat about this. I hope that you enjoyed!&lt;/p&gt;

&lt;h3 id=&quot;bibliography-and-further-reading&quot;&gt;Bibliography and further reading&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=pricing+options+in+the+vasicek+model&amp;amp;btnG=&quot;&gt;Pricing options under the Vasicek model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scholar.google.com/scholar?hl=it&amp;amp;as_sdt=0%2C5&amp;amp;q=static+hedging+of+barrier+options&amp;amp;btnG=&quot;&gt;Static hedging of barrier options&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bj√∂rk, Tomas, 2009. &lt;em&gt;‚ÄúArbitrage Theory in Continuous Time‚Äù&lt;/em&gt;. Professor Bj√∂rk passed away in 2021: although I never met him, I am still very grateful for this work of his from which I learned the fundamentals of finance in continuous time. A must-read for all who aspire to work in this field.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://warwick.ac.uk/fac/soc/wbs/subjects/finance/research/wpaperseries/2004/04-211.pdf&quot;&gt;Effects of intraday volatility on Monte Carlo estimates.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Musiela, Mark &amp;amp; Rutkowski, Marek, 2004. &lt;em&gt;‚ÄúMartingale Methods in Financial Modelling‚Äù&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;updates&quot;&gt;Updates&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;12/10/2022 - Rolled back to MathJax version 2.7 to improve mobile-friendliness: TeX sections have been modified accordingly. Fixed some typos.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Luca Bressan</name></author><category term="derivatives" /><summary type="html">Cover photo by T L. Arguably, a dam is a down-and-out type of barrier, although not the kind we‚Äôre interested in. What you should know before reading this post Fundamentals of option pricing in the Black-Scholes model. Fundamentals of stochastic calculus. About barrier options and other related contracts Let‚Äôs be honest: option pricing is not an easy feat to begin with, and the work of Black and Scholes is nothing short of magic. Today we are going to face the issue of options with path-dependent payoffs, in particular we are going to take a look at single, fixed barrier options and lookbacks. We will introduce some mathematical results that allow us to think more financially when dealing with the pricing problem, instead of getting bogged down in the maths of it. We will test some of our results with a Monte Carlo estimate, checking whether the closed form formula agrees with simulations. I suggest that you read this on a decently sized screen: MathJax is notoriously mobile-unfriendly. Rotating your device in landscape mode may help with long formulas and equations. Take a deep breath, it‚Äôs going to be kind of intenseüòÇ. Let‚Äôs begin! A closer look at the Black-Scholes equation It‚Äôs easy to lose sight of the target when dealing with an option pricing problem. That is, many forget that the option pricing problem is not about find the general integral of the parabolic PDE that is the Black-Scholes equation, but it is about finding the unique solution to a boundary constraint problem. Why does this matter so much? Let‚Äôs introduce the Black-Scholes PDE operator: for each suitable function \(f\), let \(\mathfrak{d}_{\sigma,r}(f)\) be the operator that returns the function \begin{equation} \mathfrak{d}_{\sigma,r}(f) := \frac{\partial f}{\partial t} + \frac{1}{2} \sigma^2 x^2 \frac{\partial^2 f}{\partial x^2} + rx \frac{\partial f}{\partial x} - rf \end{equation} One might see that \(\mathfrak{d}_{\sigma,r}(f)\) is a linear operator. This is of fundamental importance to our goal: two different solutions to our PDE can be combined to get a new one. Should we be able to combine different solutions to the equation to satisfy the boundary condition, we would be successful in solving the option pricing problem. Let‚Äôs introduce the crucial tools that will allow us to exploit this property. Proposition Let \begin{equation} \frac{dX}{X} = \mu dt + \sigma dW_{t} \end{equation} be the dynamic of a geometric brownian motion under a suitable probability measure. Then there exists a number \(\alpha\) such that \(X^{\alpha}\) is a martingale under the same probability measure. Proof sketch The appropriate \(\alpha\) is \(\alpha = 1 - \frac{2 \mu}{\sigma^2}\). From here one should use Ito‚Äôs formula on \(X^{\alpha}\) and see that the resulting dynamic is one of a martingale. ‚àé This result is going to be crucial in the next proof. We still need some preliminary work before diving into barrier option pricing. Definition. (Contract function) A contract function \(\Phi_{A_{T}}\) on some asset \(A_{t}\) is a generic function of at least asset price and time of expiration. For our intents and purposes, it will represent exactly the value of a given contract at expiration. Our contract functions unfortunately won‚Äôt be just function of asset price and time of expiration. They will be functions of the asset path as well, that is they will be functions of \(A_{t}\) for some \(t&amp;lt;T\). This is what we mean when we talk about path-dependency. Definition. (Pricing function, price function) A pricing function \(\pi_{\Phi_{A_{T}}, t}\) on some contract \(\Phi_{A_{T}}\) is a function that gives the no-arbitrage price of said contract at any time before expiration. Sometimes I won‚Äôt be as pedantic when using notation. No need to be as long as we‚Äôre understating each other. Be flexible! Theorem. (Principle of Reflection) Let \(\Phi_{S_{T}}\) be a contract function on some asset \(S_{t}\) that follows a GBM dynamic in a complete market free of arbitrages, and let \(\pi_{\Phi_{S_{T}}, t}\) be the associated pricing function that solves the Black-Scholes PDE \(\mathfrak{d}_{\sigma,r} ( \pi_{\Phi_{S_{T}}, t} ) = 0\). Let \(H&amp;gt;0\) and let \(\alpha\) be the number such that \(S^{\alpha}\) is a martingale under the same probability measure. Consider the contract function \(\Psi_{S_{T}}=\left( \frac{S_{T}}{H} \right) ^{\alpha} \Phi_{\frac{H^2}{S_{T}}, t}\) Then, the associated pricing function to the new claim that solves the Black-Scholes PDE is \begin{equation} \xi_{S_{t},t} = \left( \frac{S_{t}}{H} \right) ^{\alpha} \pi_{\Phi_{S_{T}}, t} \left( \frac{H^2}{S_{T}}, t \right) . \end{equation} Proof The appropriate price (that is, the pricing function that solves the Black-Scholes PDE) for \(\Psi_{S_{T}}\) given by \begin{equation} \xi=e^{-r(T-t)} \mathbb{E}^{\mathbb{Q},t} \left[ \left( \frac{S_{t}}{H} \right) ^{\alpha} \Phi_{\frac{H^2}{S_{T}}, t} \right] \end{equation} where \(\mathbb{Q}\) is the probability measure such that \(S_{t}^{\alpha}\) is a martingale. Consider \(\Psi_{S_{T}}\) to be a contract on asset \(U_{t}=\frac{H^2}{S_{t}}\). Then, use the abstract Bayes theorem to state that there exists a new probability measure \(\mathbb{B}\), obtainable via a Radon-Nikodym derivative, such that \begin{equation} \xi \left( \frac{S_{t}}{H} \right) ^{- \alpha} = e^{-r(T-t)} \mathbb{E}^{\mathbb{B},t} \left[ \Phi_{U_{T},T} \right] \end{equation} Furthermore, the following can be stated: \(dW^{\mathbb{B}}=dW^{\mathbb{Q}}-\alpha \sigma dt\). \(U\) has \(\mathbb{B}\)-dynamic \(\frac{dU}{U} = \mu dt - \sigma dW^{\mathbb{B}}\) Since the \(\mathbb{B}\)-dynamic of \(U_{t}\) and the \(\mathbb{Q}\)-dynamic of \(S_{t}\) are equivalent as the minus sign in front of the brownian differential does not impact the distribution, we may state that the right-hand side of the pricing equation is exactly \(\pi\) evaluated for the desired arguments. Multiply then both sides by \(\left( \frac{S_{t}}{H} \right) ^{\alpha}\) to conclude the proof. ‚àé Let‚Äôs add some remarks on this proof. Some of the readers might be familiar with other names for the abstract Bayes‚Äô theorem: for example it is also called change of numerary theorem, and those who are familiar with the Black‚Äôs formula for futures pricing are also aware of its immense versatility. It can be used to prove other pricing formulas, like Margrabe‚Äôs formula for exchange options. Furthermore, we are assuming constant volatility, constant risk-free rate and no dividend yield. While accounting for deterministically varying dividends, rates and volatility is relatively easy (see Bj√∂rk), accounting for random variations in this parameters is much more tedious and may preclude the possibility of finding a closed form for the prices (see Google Scholar: pricing options under the Vasicek interest rate model). Let‚Äôs move on: why did we bother with the reflection theorem in the first place? The assets \(S_{t}\) and \(U_{t}\) have a peculiar property: they are always on opposite sides of the number \(H\), and they match when \(S_{t}=H\). By this point you may have already correctly guessed that \(H\) is our barrier. But this is where the real magic happens: consider a vanilla call option struck at \(K\). The reflection principle allows us to synthesize a new contract (from now on called reflected) that has the exact same price of the original one when \(S_{t}=H\), independently of time. This is the key point of what we‚Äôre about to do. Pricing a down-and-out regular call option Consider a down-and-out call option struck at \(K\) where the barrier \(H\) is out-of-the-money (this last condition is what distinguishes regular barrier options from reverse barrier options). The pricing problem is a boundary constrained problem, built on the Black-Scholes PDE. The constraints are: terminal pricing condition: the price at expiry must match the payoff of the contract; lack-of-optionality condition: when the asset price is very high, our option must be treated more and more like a futures contract, since the benefit they provide is equivalent; barrier condition: whenever the underlying asset touches the barrier, the option ceases to exist, turning into nothingness. And nothing has no value. We claim that a portfolio made by going long on a vanilla call with strike \(K\) and going short on the reflection of such call option is going to provide the solution to our pricing problem: at t=T, our reflected call is always going to be out-of-the-money, as the barrier was not touched. One can see that by observing that \(\frac{H^2}{S_{T}} &amp;lt; \frac{H^2}{H} = H &amp;lt; K\). This means that the portfolio value is entirely given by our vanilla call, as intended; as \(S_{t}\) goes to infinity, so \(\frac{H^2}{S_{T}}\) goes to zero, meaning the the reflected call has negligible value and our portfolio inherits its lack-of-optionality from the vanilla call; independently of time, whenever \(S_{t}\) equals \(H\) the reflected call value cancels out the vanilla call value, giving a net value to the portfolio equal to zero. An important consideration must be added: our process works under the assumption that our contract function has an indicator function to nullify its value whenever the barrier is touched. In symbols, we are assuming our contract function to look like this \(\xi_{(...)} = \tilde{\xi}_{(...)} \mathbb{1}_{\lbrace \forall t : S_{t}&amp;gt;H \rbrace}\): this is what allows us not to worry about what happens when \(S_{t}\) is below the barrier. We can now state what the price for this barrier option is: \begin{equation} C^{DOC,regular} (S_{t}, t, \sigma, r, H, K) = C^{vanilla} \left( S_{t}, t, \sigma, r, K \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) - \left( \frac{S_{t}}{H} \right)^{\alpha} C^{vanilla} \left( \frac{H^2}{S_{t}}, t, \sigma, r, K \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) \end{equation} Unsurprisingly, we observe that the barrier clause generates a discount on the final price. Evolution of price functions for a plain vanilla call (in yellow) and a DOC regular option (in blue). Notice how the blue line always lies below the yellow line. In red, the payoff at maturity. Pricing a down-and-out reverse call When the barrier is in-the-money, our barrier condition is no longer satisfied if we just use the previous approach. We need to make some minor alterations to the options portfolio we intend to reflect. We should build a portfolio of derivatives whose value is zero at the barrier, while having exactly the value of a vanilla call option whenever the underlying asset price is higher. This is obtained by going long on one vanilla call struck at \(H\) and \(H-K\) units of a digital call also struck at \(H\). Reflect this portfolio to obtain the following pricing formula: \begin{equation} C^{DOC,reverse} (S_{t}, t, \sigma, r, H, K) = C^{vanilla} \left( S_{t}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) + (H-K) C^{digital} \left( S_{t}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) - \left( \frac{S_{t}}{H} \right)^{\alpha} C^{vanilla} \left( \frac{H^2}{S_{t}}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) - (H-K) \left( \frac{S_{t}}{H} \right)^{\alpha} C^{digital} \left( \frac{H^2}{S_{t}}, t, \sigma, r, H \right) \mathbb{1}_{ \left\lbrace S_{t}&amp;gt;H \right\rbrace } (S_{t}) \end{equation} Maturity payoff of the truncated portfolio. Solid red line indicates the contract function, dashed black line indicates the barrier threshold and dot-dashed blue line indicates the strike price of the option. In-Out parities Whenever a portfolio is made up of knock-out barrier option and the respective knock-in counterpart, the resulting portfolio is a plain vanilla option of the same type. While this is intuitively clear, a quick algebraic manipulation shows that this fact is also mathematically correct. Feel free to have a try proving this, it‚Äôs easy. It is just a matter of being careful with indicator functions algebra. This allows us to price the knock-in contracts given the knock-out price and viceversa. Pricing an up-and-out regular put, and pretty much anything else The reflection principle is still our primary tool when it comes to pricing up contracts. Our approach is still valid even with our new boundary constraints. Namely, our spatial constraints define a bounded region in the set of possible \(S_{t}\) values (the term spatial constraint comes from thermodynamics). What we did by imposing the knock-out clause is essentialy setting an upper bound to the values that \(S_{t}\) can take without nullifying our contract. A lower bound was already present, since in our model asset prices cannot be negative. It is left to the reader to try out every possibile combination of put, call, knock-in, knock-out regular and reverse contract. This approach applies to pretty much every contract function as long as the barrier is unique and fixed. Let‚Äôs close this discussion with a quick question: There are barrier contracts which are exactly equivalent to their vanilla counterparts, which ones? Think of barrier contracts which have zero value no matter the time to expiration or the value of the underlying. Testing our formulas It would be rather useful to check whether our previous work is correct by comparing it with something that is certainly correct. We shall do this by comparing the result of our formulas with the result of a Monte Carlo estimation. Our simulation will be done using MATLAB. Consider an hypotethical asset-market combination with the following properties: \(S_{0}=100 \$\), \(r=2.5 \%\), \(\sigma = 20\%\). We will try pricing a down-and-out regular call with \(H=95\), \(T=1\) and \(K=102.5 \$\). It is essentially an ATMF call option with a knock-out clause. We will simulate 30000 paths: unfortunately Monte Carlo methods are slow to converge. Let‚Äôs take a look at the MATLAB code: % Pricing parameters mu = 0.025; sigma = 0.2; s_0 = 100; deltaTime=1/(252*8*12); numDataPoints=252*8*12; numPaths=30000; K=102.5; H=95; alpha=1-2*(mu/(sigma^2)); % Asset paths simulation asset = gbm(mu,sigma,&apos;StartState&apos;,s_0); tic; [X, T] = asset.simByEuler(numDataPOints, &apos;DeltaTime&apos;, deltaTime, &apos;nTrials&apos;, numPaths); toc assetPaths=zeros(numDataPoints+1,numPaths); for i=1:numPaths assetPaths(:,i)=X(:,:,i); end % Option price estimation optionSimulatedPayoffs=zeros(1,numPaths); for k=1:numPaths if min(assetPaths(:,k))&amp;lt;=H optionSimulatedPayoffs(k)=0; else optionSimulatedPayoffs=max(assetPaths(end,k)-K, 0); end end optionEstimatedPrice=exp(-mu)*mean(optionSimulatedPayoffs); optionExactPrice=blsprice(s_0,K,mu,1,sigma) - ((s_0/H)^alpha).*blsprice(H^2/s_0,K,mu,1,sigma); The code is very much self explainatory, and for details about stochastic processes simulation one should refer to the official documentation by MathWorks. One might wonder why the number of data points is such that we are considering observations taken every quarter of an hour over a day (or taken every five minutes over an 8-hours working day). If we were to consider observations taken at the end of each trading day, we would be giving an overly optimistic and biased estimate of the option price as we would be discarding effects due to intraday volatility. To make a long story short, our asset may breach the barrier in between observation points, and we wouldn‚Äôt be able to notice that. Our estimated price is \(V=4.39(8)\$\). Assuming an approximately normal distribution for the estimator (can you spot any issues with this?) we get a standard error of \(0.06(8) \$\) and our theoretical price of \(4.34(5) \$\) is within the 95% confidence interval of our estimation. We can conclude that our Monte-Carlo estimate and our closed-form analytical formula agree on the result to a sufficient degree that we can confidently assign a price to the option. A sampling of 100 simulated asset paths. The red line marks the average of the simulated asset paths. In blue it is shown the risk-free growth of a capital equal to the spot price of the underlying. Static hedging of barrier options Once the appropriate formula for a price is found, it is natural to ask ourselves how to hedge a position involving barrier options. Unfortunately, when taking derivatives to evaluate \(\Delta\), the indicator functions induce finite jump discontinuities in the result. This means that hedging a position when the underlying is near the barrier can be extremely problematic, and static hedging techniques are out of the scope of this article. In the last 20 years the literature about solving this issue has been very abundant, so a search on Google Scholar can be a good starting point for learning more. Price and delta as a function of the underlying asset price for some DIC option. Notice the cusp in the price function and discontinuity at the barrier in the delta. Getting creative: pricing a floating strike lookback call At the heart of our work lies the issue of path-dependency. That is, our contract function is not only a function of information known at the time of expiration, but it also depends on other times preceding the expiration. Our clever solution with reflections had the only goal of removing path-dependency from our calculations. Now we will take a look at another contract that exhibits path dependency. Floating strike lookback calls are options that allow us to buy an asset at the lowest price recorded in between issuance of the option and expiration. Our contract function looks like this: \begin{equation} \Phi = S_{T} - \min_{t= \tau} S_{\tau} \end{equation} We will obtain a pricing formula decomposing this contract in a portfolio of options that are either not path-dependent or whose path-dependency we can manage. Let \(t\) be some time before expiration. Let \(\tau(t)\) be the time where our asset reached it lowest price, that is let \(\tau(t) = \arg \min _{t} S_{t}\). Intuitively, our portfolio must start with a plain vanilla call option, struck at \(S_{\tau(t)}\), given that surely we‚Äôll have the right to buy the asset at such price. Then, our portfolio must take into account the possibility that the asset will fall even lower than \(S_{\tau(t)}\). We will add something to our portfolio that satisfies the following requirements: we must be able to price it using reflections; no benefit must be provided at expiration if \(S_{\tau(t)} = S_{\tau(T)}\); the resulting portfolio must be a call struck at \(S_{\tau(T)}\) (i.e. our lookback option). We claim that this result can be achieved by adding infinitely many contracts, each one of them being bought in infinitesimal quantity. Consider for any \(\gamma\) lesser than \(S_{\tau(t)}\), a position obtained by going short \(d\gamma\) units of a down-and-in digital call struck at \(S_{\tau(t)}\) with barrier at \(\gamma\) and going long by the same amount on a down and in digital call struck at \(\gamma\) with barrier at \(\gamma\). Let‚Äôs check our claim: we can price digital DICs using reflections; since any \(\gamma\) is lesser than \(S_{\tau(t)}\), if \(S_{\tau(t)} = S_{\tau(T)}\) none of our contracts are active, and the payoff is null. suppose that \(S_{T}\) is greater than \(S_{\tau(t)}\). Integrate the payoff over the active, non zero value contract region (i.e. between \(S_{\tau(t)}\) and \(S_{\tau(T)}\)): \(\int_{S_{\tau(T)}}^{S_{\tau(t)}} d\gamma = S_{\tau(t)} - S_{\tau(T)}\). Then add to our previous result the payoff of our plain vanilla call: it is \(S_{T}-S_{\tau(t)}\), and therefore we obtain the right quantity. If \(S_{T}\) is lesser than \(S_{\tau(t)}\), the above integral reduces to \(\int_{S_{\tau(T)}}^{S_{T}} d\gamma = S_{T} - S_{\tau(T)}\), and adding the null payoff of the plain vanilla call yields again the correct result. All of this allows us to state that the the price for a lookback floating strike call is given by \begin{equation} \int_0^{S_{\tau(t)}} \left( C^{DIC,digital}(S_{t}, t, \sigma, r, \gamma, \gamma) - C^{DIC,digital}(S_{t}, t, \sigma, r, \gamma, S_{\tau(t)} \right) d\gamma + C^{call}(S_{t}, t, \sigma, r, S_{\tau(t)}) \end{equation} You‚Äôll be delighted to know that this integral allows for a closed form analytical solution, but solving the integral requires a certain bravery, a bit of time and a ton of paper. If you‚Äôre up for a nice integration challenge, go for it. You will find out that the solution is \begin{equation} S_{t}e^{-r(T-t)}N(d_{1})-S_{\tau(t)}e^{-r(T-t)}N(d_{2})+S_{t}e^{-r(T-t)} \sigma \sqrt{T-t} \left[ \frac{1}{\sqrt{2\pi}} e^{- \frac{d_{1}^2}{2}}+d_{1}N(-d_{1}) \right] \end{equation} where \(N(x)\) is the standard gaussian cdf. and \(d_{1}\) and \(d_{2}\) are the usual Black-Scholes quantiles evaluated for \(K=S_{\tau(t)}\) and \(S=S_{t}\). This expression is somewhat more complex if we allow for the presence of dividends, but the reasoning remains the same. Again feel free to try, I‚Äôm sure you‚Äôll manage as it‚Äôs mostly a matter of making the right choices while integrating by parts. Conclusions We have seen how the reflection principle allows us to get rid of path-dependency and lets us find pricing formula that would be otherwise much more tedious to evaluate. This is not the only way possible: you may want to look into the theory of absorbed processes. This is the way that these formula were first found (see Muisela and Bj√∂rk), and it is a perfecty valid one, although much more mathematically intense. The beauty of the reflection principle lies in the possibilty of being able to reason in financial terms, and the math just gets out of the way. This article is a good starting point, but much more can be said about path-dependent contract functions. Some possible next steps from here: looking into more contracts, such as ladders and double barriers; looking at how things change when we have stochastic dividends and rates; looking at the classic heat diffusion model (interestingly, the option pricing problem is very similar to the problem of predicting temperature evolution across an infinitely long rod), and then looking at the heat diffusion model with constant temperature zones (you will find that they behave very much like our barriers). This is my first article for this blog, and my first attempt at blogging in general. Reach out to me on Linkedin or GitHub if you want to suggest improvements or if you want to just have a chat about this. I hope that you enjoyed! Bibliography and further reading Pricing options under the Vasicek model Static hedging of barrier options Bj√∂rk, Tomas, 2009. ‚ÄúArbitrage Theory in Continuous Time‚Äù. Professor Bj√∂rk passed away in 2021: although I never met him, I am still very grateful for this work of his from which I learned the fundamentals of finance in continuous time. A must-read for all who aspire to work in this field. Effects of intraday volatility on Monte Carlo estimates. Musiela, Mark &amp;amp; Rutkowski, Marek, 2004. ‚ÄúMartingale Methods in Financial Modelling‚Äù. Updates 12/10/2022 - Rolled back to MathJax version 2.7 to improve mobile-friendliness: TeX sections have been modified accordingly. Fixed some typos.</summary></entry></feed>